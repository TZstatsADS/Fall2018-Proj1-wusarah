---
title: "Happy Moments - Project #1"
author: "Sarah Wu"
output:
  html_document:
    df_print: paged
---

###Part 1: Text Processing 
*From Starter code
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

HappyDB is a corpus of 100,000 crowd-sourced happy moments via Amazon's Mechanical Turk. You can read more about it on https://arxiv.org/abs/1801.07746

In this R notebook, we process the raw textual data for our data analysis.

### Step 0 - Load all the required libraries

From the packages' descriptions:

+ `tm` is a framework for text mining applications within R;
+ `tidyverse` is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures;
+ `tidytext` allows text mining using 'dplyr', 'ggplot2', and other tidy tools;
+ `DT` provides an R interface to the JavaScript library DataTables.

```{r load libraries, warning=FALSE, message=FALSE, echo=FALSE}
packages.used=c("dplyr", "tidytext", "tidyverse", "DT","tm", "scales", "wordcloud",
                "topicmodels", "ngram", "gridExtra", "ggplot2")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}

#load packages
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
library(dplyr)
library(topicmodels)
library(ggplot2)
library(scales)
library(wordcloud)
library(gridExtra)
library(ngram)
```


```{r, echo=FALSE}
#print my version of R
print(R.version)
```

### Step 1 - Load the data to be cleaned and processed

```{r read data, warning=FALSE, message=FALSE}
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
```

### Step 2 - Preliminary cleaning of text

We clean the text by converting all the letters to the lower case, and removing punctuation, numbers, empty words and extra white space.

```{r text processing in tm, echo=FALSE}
corpus <- VCorpus(VectorSource(hm_data$cleaned_hm))%>%
  tm_map(content_transformer(tolower))%>%
  tm_map(removePunctuation)%>%
  tm_map(removeNumbers)%>%
  tm_map(removeWords, character(0))%>%
  tm_map(stripWhitespace)

```

### Step 3 - Stemming words and converting tm object to tidy object

Stemming reduces a word to its word *stem*. We stem the words here and then convert the "tm" object to a "tidy" object for much faster processing.
```{r stemming, echo=FALSE}
stemmed <- tm_map(corpus, stemDocument) %>%
  tidy() %>%
  select(text)

```


### Step 4 - Creating tidy format of the dictionary to be used for completing stems

We also need a dictionary to look up the words corresponding to the stems.

```{r tidy dictionary, echo=FALSE}
dict <- tidy(corpus) %>%
  select(text) %>%
  unnest_tokens(dictionary, text)

```

### Step 5 - Removing stopwords that don't hold any significant information for our data set

We remove stopwords provided by the "tidytext" package and also add custom stopwords in context of our data.

```{r stopwords, echo=FALSE}
data("stop_words")

word <- c("happy","ago","yesterday","lot","today","months","month",
                 "happier","happiest","last","week","past")

stop_words <- stop_words %>%
  bind_rows(mutate(tibble(word), lexicon = "updated"))
```

### Step 6 - Combining stems and dictionary into the same tibble

Here we combine the stems and the dictionary into the same "tidy" object.

```{r tidy stems with dictionary, echo=FALSE}
completed <- stemmed %>%
  mutate(id = row_number()) %>%
  unnest_tokens(stems, text) %>%
  bind_cols(dict) %>%
  anti_join(stop_words, by = c("dictionary" = "word"))

completed

```

### Step 7 - Stem completion

Lastly, we complete the stems by picking the corresponding word with the highest frequency.

```{r stem completion, warning=FALSE, message=FALSE, echo=FALSE}
completed <- completed %>%
  group_by(stems) %>%
  count(dictionary) %>%
  mutate(word = dictionary[which.max(n)]) %>%
  ungroup() %>%
  select(stems, word) %>%
  distinct() %>%
  right_join(completed) %>%
  select(-stems)

completed
```

### Step 8 - Pasting stem completed individual words into their respective happy moments

We want our processed words to resemble the structure of the original happy moments. So we paste the words together to form happy moments.

```{r reverse unnest, echo=FALSE}
completed <- completed %>%
  group_by(id) %>%
  summarise(text = str_c(word, collapse = " ")) %>%
  ungroup()

completed
```

### Step 9 - Keeping a track of the happy moments with their own ID

```{r cleaned hm_data, warning=FALSE, message=FALSE, echo=FALSE}
hm_data <- hm_data %>%
  mutate(id = row_number()) %>%
  inner_join(completed)

datatable(hm_data)
```

### Exporting the processed text data into a CSV file

```{r export data}
write_csv(hm_data, "../output/processed_moments.csv")
```
### Step 1 - Load the processed text data along with demographic information on contributors

We use the processed data for our analysis and combine it with the demographic information available.

```{r load data, warning=FALSE, message=FALSE}
hm_data <- read_csv("../output/processed_moments.csv")

urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
```

### Combine both the data sets and keep the required columns for analysis

We select a subset of the data that satisfies specific row conditions.

```{r combining data, warning=FALSE, message=FALSE}
hm_data <- hm_data %>%
  inner_join(demo_data, by = "wid") %>%
  select(wid,
         original_hm,
         gender, 
         marital, 
         parenthood,
         reflection_period,
         age, 
         country, 
         ground_truth_category, 
         predicted_category,
         text) %>%
  mutate(count = sapply(hm_data$text, wordcount)) %>%
  filter(gender %in% c("m", "f")) %>%
  filter(marital %in% c("single", "married")) %>%
  filter(parenthood %in% c("n", "y")) %>%
  filter(reflection_period %in% c("24h", "3m")) %>%
  mutate(reflection_period = fct_recode(reflection_period, 
                                        months_3 = "3m", hours_24 = "24h"))

```
Check the data for correct data types, outliers, and possible errors
```{r}
summary(hm_data)

hm_data2 <- hm_data
hm_data2[,c(3:5,8:10)] <- as.data.frame(sapply(hm_data[,c(3:5,8:10)], as.factor))
summary(hm_data2)

#Since we're focusing on an age group, let's clean up age
hm_data2$age <- as.numeric(hm_data2$age)
summary(hm_data2)
#Now we have the proper data types for our variables
#Other observations:
#1. Respondents are more male than female
#2. More single than married
#3. More non-parents than parents
#4 near equal split between reflection periods
#5 large majority from the US, followed by India
#6 Most happy moments fall under the categories of achievement, affection, bonding, and enjoy_the_moment


hist(hm_data2$age)

#There appear to be strange outliers (over 100 years old and as young as 2 years old) - which seem unlikely to be legitimate.
hm_data2 <- hm_data2[which(hm_data2$age<100 & hm_data2$age>5),]
hist(hm_data2$age)
#Majority of respondents are within the 20-40 age range.

hist(hm_data2$count)


```

For this analysis, I want to focus on a personal curiosity - for peers within my age group (26-30), what brings them happiness? How do males and females differ in this regard?
```{r}
hm_data2$peer_agegroup <- as.factor(ifelse((hm_data2$age>25&hm_data2$age<31),"peer", "non-peer"))
summary(hm_data2)


summary(hm_data2%>%
          filter(hm_data2$peer_agegroup=="peer"))
#as a whole, it does not appear that those aged 26-30 have very different happiness categories vs the whole group - roughly 2/3 still fall under achievement and affection
```


### Create a bag of words using the text data

```{r bag of words, warning=FALSE, message=FALSE}
bag_of_words <-  hm_data2 %>%
  unnest_tokens(word, text)

#Let's see the word frequencies in descending order
word_count <- bag_of_words %>%
  count(word, sort = TRUE)

##Plot
barplot(word_count[1:20,]$n, las = 2, names.arg = word_count[1:20,]$word,
        col ="lightblue", main ="Most frequent words",
        ylab = "Word frequencies")

#for peer group
bag_of_words_peer <-  hm_data2 %>%
  dplyr::filter(hm_data2$peer_agegroup=="peer")%>%
  unnest_tokens(word, text)

#Let's see the word frequencies in descending order
word_count_peer <- bag_of_words_peer %>%
  dplyr::group_by(bag_of_words_peer$gender)%>%
  count(word, sort = TRUE)

####how to show this by group
barplot(word_count_peer[1:20,]$n, las = 2, names.arg = word_count_peer[1:20,]$word,
        col ="lightblue", main ="Most frequent words",
        ylab = "Word frequencies")

total_words <- sum(word_count$n)
total_words_peer <- sum(word_count_peer$n)

freq_by_rank <- word_count %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total_words)

freq_by_rank

freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()

plot(hm_data2$gender, hm_data2$predicted_category)
```



```{r}
#Here I attempt to see any significance from a TF-IDF analysis, but looking at the output it does not seem that meaningful. Many of the words appear to be misspelled - "selfit", "promotionit", "ruut", etc. and some don't even look like english - "thekkady"
book_words <- word_count_peer %>%
  bind_tf_idf(word, bag_of_words_peer$gender, n)

book_words %>%
  select(-total_words) %>%
  arrange(desc(tf_idf))

book_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(`bag_of_words_peer$gender`) %>% 
  top_n(15) %>% 
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = `bag_of_words_peer$gender`)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~`bag_of_words_peer$gender`, ncol = 2, scales = "free") +
  coord_flip()

#Only a handful of words appear meaningful:
#f: mommy, ebook
#m: split, prominent, nba, gf, flourishing, experiential, bitcoin
```

### Create bigrams using the text data

```{r bigram, warning=FALSE, message=FALSE}
hm_bigrams <- hm_data2 %>%
  filter(count != 1) %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

bigram_counts <- hm_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  count(word1, word2, sort = TRUE)

bigram_counts


hm_bigrams_peer <- hm_data2 %>%
  dplyr::filter(hm_data2$peer_agegroup=="peer")%>%
  filter(count != 1) %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

bigram_counts_peer <- hm_bigrams_peer %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  count(word1, word2, sort = TRUE)

bigram_counts_peer_f <- hm_bigrams_peer%>%
  dplyr::filter(hm_bigrams_peer$gender=="f")%>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  count(word1, word2, sort = TRUE)

bigram_counts_peer_m <- hm_bigrams_peer%>%
  dplyr::filter(hm_bigrams_peer$gender=="m")%>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  count(word1, word2, sort = TRUE)

#the #1 bi-gram for men aged 26-30 is... video games! Additionally, #9 is "played video" [likely games]

bigram_counts_peer_m
bigram_counts_peer_f

bigram_counts_mothers <- hm_bigrams %>%
  dplyr::group_by(hm_bigrams$gender)%>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  count(word1, word2, sort = TRUE)

```


```{r}

set.seed(123)
wordcloud(words = word_count$word, freq = word_count$n, min.freq = 100, max.words = 100, random.order = FALSE, colors = brewer.pal(8, "Dark2"))


wordcloud(words = word_count_peer$word, freq = word_count_peer$n, min.freq = 100, max.words = 100, random.order = FALSE, colors = brewer.pal(8, "Dark2"))


class(bag_of_words$age)
class(hm_data)



#pairwise correlation
#associated words
```



```{r}

dtm <- cast_dtm(data=word_count_peer, term = word, document = bag_of_words_peer$gender, value = n)

hm_lda <- LDA(dtm, k=2, control=list(seed=1234))
summary(word_count_peer)
hm_lda
hm_topics <- tidy(hm_lda, matrix="beta")
hm_topics

hm_top_terms <- hm_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

hm_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

beta_spread <- hm_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

beta_spread <- beta_spread[order(beta_spread$log_ratio),]
dim(beta_spread)
beta_spread_a <- beta_spread[1:10,]
beta_spread_b <- beta_spread[258:267,]
beta_spread_fin <- rbind(beta_spread_a, beta_spread_b)
beta_spread_fin
ggplot(data = beta_spread_fin, aes(y=beta_spread_fin$log_ratio, x=beta_spread_fin$term)) + geom_bar(stat='identity')+  coord_flip()

ggplot(data = beta_spread_fin, aes(y=beta_spread_fin$log_ratio, x=reorder(beta_spread_fin$term,-beta_spread_fin$log_ratio))) + geom_bar(stat='identity', position='dodge') +coord_flip()
```
Things I want to do:
1. show word frequencies by gender, parenthood, etc
2. show bi-grams by gender, etc.
3. plot overall word freq
4. plot highest freq bi-grams
