---
title: "Happy Moments - Project #1"
author: "Sarah Wu"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r load libraries, warning=FALSE, message=FALSE, echo=FALSE}
packages.used=c("dplyr", "tidytext", "tidyverse", "DT","tm", "scales", "wordcloud",
                "topicmodels", "ngram", "gridExtra", "ggplot2")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}

#load packages
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
library(dplyr)
library(topicmodels)
library(ggplot2)
library(scales)
library(wordcloud)
library(gridExtra)
library(ngram)
```




```{r read data, warning=FALSE, message=FALSE, echo=FALSE}
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
```


```{r text processing in tm, echo=FALSE}
corpus <- VCorpus(VectorSource(hm_data$cleaned_hm))%>%
  tm_map(content_transformer(tolower))%>%
  tm_map(removePunctuation)%>%
  tm_map(removeNumbers)%>%
  tm_map(removeWords, character(0))%>%
  tm_map(stripWhitespace)

```


```{r stemming, echo=FALSE}
stemmed <- tm_map(corpus, stemDocument) %>%
  tidy() %>%
  select(text)

```



```{r tidy dictionary, echo=FALSE}
dict <- tidy(corpus) %>%
  select(text) %>%
  unnest_tokens(dictionary, text)

```


```{r stopwords, echo=FALSE}
data("stop_words")

word <- c("happy","ago","yesterday","lot","today","months","month",
                 "happier","happiest","last","week","past")

stop_words <- stop_words %>%
  bind_rows(mutate(tibble(word), lexicon = "updated"))
```



```{r tidy stems with dictionary, echo=FALSE}
completed <- stemmed %>%
  mutate(id = row_number()) %>%
  unnest_tokens(stems, text) %>%
  bind_cols(dict) %>%
  anti_join(stop_words, by = c("dictionary" = "word"))

```


```{r stem completion, warning=FALSE, message=FALSE, echo=FALSE}
completed <- completed %>%
  group_by(stems) %>%
  count(dictionary) %>%
  mutate(word = dictionary[which.max(n)]) %>%
  ungroup() %>%
  select(stems, word) %>%
  distinct() %>%
  right_join(completed) %>%
  select(-stems)

```


```{r reverse unnest, echo=FALSE}
completed <- completed %>%
  group_by(id) %>%
  summarise(text = str_c(word, collapse = " ")) %>%
  ungroup()

```


```{r cleaned hm_data, warning=FALSE, message=FALSE, echo=FALSE}
hm_data <- hm_data %>%
  mutate(id = row_number()) %>%
  inner_join(completed)

```


```{r export data, echo=FALSE}
write_csv(hm_data, "../output/processed_moments.csv")
```


```{r load data, warning=FALSE, message=FALSE, echo=FALSE}
hm_data <- read_csv("../output/processed_moments.csv")

urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
```


```{r combining data, warning=FALSE, message=FALSE, echo=FALSE}
hm_data <- hm_data %>%
  inner_join(demo_data, by = "wid") %>%
  select(wid,
         original_hm,
         gender, 
         marital, 
         parenthood,
         reflection_period,
         age, 
         country, 
         ground_truth_category, 
         predicted_category,
         text) %>%
  mutate(count = sapply(hm_data$text, wordcount)) %>%
  filter(gender %in% c("m", "f")) %>%
  filter(marital %in% c("single", "married")) %>%
  filter(parenthood %in% c("n", "y")) %>%
  filter(reflection_period %in% c("24h", "3m")) %>%
  mutate(reflection_period = fct_recode(reflection_period, 
                                        months_3 = "3m", hours_24 = "24h"))

```

```{r, echo=FALSE}
#We check the data for correct data types, outliers, and possible errors
hm_data2 <- hm_data
hm_data2[,c(3:5,8:10)] <- as.data.frame(sapply(hm_data[,c(3:5,8:10)], as.factor))

#Since we're focusing on an age group, let's clean up age
hm_data2$age <- as.numeric(hm_data2$age)

#Now we have the proper data types for our variables
#Other observations:
#1. Respondents are more male than female
#2. More single than married
#3. More non-parents than parents
#4 near equal split between reflection periods
#5 large majority from the US, followed by India
#6 Most happy moments fall under the categories of achievement, affection, bonding, and enjoy_the_moment


#There appear to be strange outliers (over 100 years old and as young as 2 years old) - which seem unlikely to be legitimate.
hm_data2 <- hm_data2[which(hm_data2$age<100 & hm_data2$age>5),]

#Majority of respondents are within the 20-40 age range.

```
#Intro
For this analysis on HappyDB, I wanted to focus on a personal curiosity - for peers within my age group (26-30), what brings them happiness? There have been myths that claim that girls mature or "reach adulthood" a few years earlier than boys. 

![](C:/Users/Sarah Wu/Documents/GitHub/Fall2018-Proj1-wusarah/figs/Millenials.png)

Can we make some inferences from happy moments of millenials? How do males and females differ in this regard?

```{r, echo=FALSE}
hm_data2$peer_agegroup <- as.factor(ifelse((hm_data2$age>25&hm_data2$age<31),"peer", "non-peer"))


#summary(hm_data2%>% filter(hm_data2$peer_agegroup=="peer"))
#as a whole, it does not appear that those aged 26-30 have very different happiness categories vs the whole group - roughly 2/3 still fall under achievement and affection

plot(hm_data2$gender, hm_data2$predicted_category)
```
Overall, it appears that females place more happiness value in bonding than achievement, while men place achievement first.

#What words occur most frequently?

```{r bag of words, warning=FALSE, message=FALSE, echo=FALSE}
bag_of_words <-  hm_data2 %>%
  unnest_tokens(word, text)

#Let's see the word frequencies in descending order
word_count <- bag_of_words %>%
  count(word, sort = TRUE)

##Plot
barplot(word_count[1:20,]$n, las = 2, names.arg = word_count[1:20,]$word,
        col ="lightblue", main ="Most frequent words",
        ylab = "Word frequencies")

#for peer group
bag_of_words_peer <-  hm_data2 %>%
  dplyr::filter(hm_data2$peer_agegroup=="peer")%>%
  unnest_tokens(word, text)

#Let's see the word frequencies in descending order
word_count_peer <- bag_of_words_peer %>%
  dplyr::group_by(bag_of_words_peer$gender)%>%
  count(word, sort = TRUE)

word_count_f <- bag_of_words_peer%>%
  dplyr::filter(bag_of_words_peer$gender=="f")%>%
  count(word, sort = TRUE)

barplot(word_count_f[1:20,]$n, las = 2, names.arg = word_count_f[1:20,]$word, 
        col ="lightblue", main ="Most frequent words-Female 26-30",
        ylab = "Word frequencies")

word_count_m <- bag_of_words_peer%>%
  dplyr::filter(bag_of_words_peer$gender=="m")%>%
  count(word, sort = TRUE)

barplot(word_count_m[1:20,]$n, las = 2, names.arg = word_count_m[1:20,]$word, 
        col ="lightblue", main ="Most frequent words-Male 26-30",
        ylab = "Word frequencies")

```
Friends, day, and time all feature heavily for both sexes, but females have "husband" as the #4 word, while for males it's "played"; Wife does not appear until #10



```{r, echo=FALSE}

total_words <- sum(word_count$n)
total_words_peer <- sum(word_count_peer$n)

freq_by_rank <- word_count %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total_words)


```


#Selecting words by relative importance
```{r, echo=FALSE}
#Here I attempt to see any significance from a TF-IDF analysis, but looking at the output it does not seem that meaningful. Many of the words appear to be misspelled - "selfit", "promotionit", "ruut", etc. and some don't even look like english - "thekkady"
book_words <- word_count_peer %>%
  bind_tf_idf(word, bag_of_words_peer$gender, n)

book_words %>%
  select(-total_words) %>%
  arrange(desc(tf_idf))

book_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(`bag_of_words_peer$gender`) %>% 
  top_n(15) %>% 
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = `bag_of_words_peer$gender`)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~`bag_of_words_peer$gender`, ncol = 2, scales = "free") +
  coord_flip()

#Only a handful of words appear meaningful:
#f: mommy, ebook
#m: split, prominent, nba, gf, flourishing, experiential, bitcoin
```

#What if we look at multiple words or words that occur together? (bigrams)

```{r bigram, warning=FALSE, message=FALSE, echo=FALSE}
hm_bigrams <- hm_data2 %>%
  filter(count != 1) %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

bigram_counts <- hm_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  count(word1, word2, sort = TRUE)

hm_bigrams_peer <- hm_data2 %>%
  dplyr::filter(hm_data2$peer_agegroup=="peer")%>%
  filter(count != 1) %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

bigram_counts_peer <- hm_bigrams_peer %>%
  dplyr::group_by(hm_bigrams_peer$gender)%>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  count(word1, word2, sort = TRUE)

bigram_counts_peer


bigram_counts_peer_f <- hm_bigrams_peer%>%
  dplyr::filter(hm_bigrams_peer$gender=="f")%>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  count(word1, word2, sort = TRUE)

bigram_counts_peer_f

bigram_counts_peer_m <- hm_bigrams_peer%>%
  dplyr::filter(hm_bigrams_peer$gender=="m")%>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  count(word1, word2, sort = TRUE)

bigram_counts_peer_m


```

The #1 bi-gram for men aged 26-30 is... video games! Additionally, "played video" and "played games" appears as well.

![](C:/Users/Sarah Wu/Documents/GitHub/Fall2018-Proj1-wusarah/figs/videogame.jpg)


```{r, echo=FALSE} 
bigrams_united <- bigram_counts_peer %>%
  unite(bigram, word1, word2, sep = " ")

bigram_tf_idf <- bigrams_united %>%
  bind_tf_idf(bigram, hm_bigrams_peer$gender, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf %>%
  select(-total_words_peer) %>%
  arrange(desc(tf_idf))
bigram_tf_idf

bigram_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>% 
  group_by(`hm_bigrams_peer$gender`) %>% 
  top_n(15) %>% 
  ungroup %>%
  ggplot(aes(bigram, tf_idf, fill = `hm_bigrams_peer$gender`)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~`hm_bigrams_peer$gender`, ncol = 2, scales = "free") +
  coord_flip()

```
But if we look at the bigrams from their relative importance to the document, the results are somewhat different. Promotion, living life, and dating girlfriend takes the lead for men, while husband surprise is #1 for women.

#Topic Modelling
```{r, echo=FALSE}
#Topic Modelling

dtm <- cast_dtm(data=word_count_peer, term = word, document = bag_of_words_peer$gender, value = n)

hm_lda <- LDA(dtm, k=2, control=list(seed=1234))
hm_topics <- tidy(hm_lda, matrix="beta")

hm_top_terms <- hm_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

hm_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()


```

#Words with greatest differences between 2 groups:
```{r}
beta_spread <- hm_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

beta_spread <- beta_spread[order(beta_spread$log_ratio),]
beta_spread_a <- beta_spread[1:10,]
beta_spread_b <- beta_spread[258:267,]
beta_spread_fin <- rbind(beta_spread_a, beta_spread_b)

ggplot(data = beta_spread_fin, aes(y=beta_spread_fin$log_ratio, x=reorder(beta_spread_fin$term,-beta_spread_fin$log_ratio))) + geom_bar(stat='identity', position='dodge') +coord_flip()
```

#Summary:

1. While playing video games appears prominently among the happy moments of men aged 26-30, they draw similar happiness from promotions/achievements and moments of affection (dating their girlfriends or their wives giving birth)

2. For women, family influences happy moments heavily. Husbands, sons, and daughters come up more than I initially expected, whereas the word "boyfriend" occurs far less frequently